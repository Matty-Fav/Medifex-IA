import streamlit as st
import pandas as pd
import numpy as np
import joblib
from datetime import datetime
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.model_selection import train_test_split, GridSearchCV # GridSearchCV pour l'optimisation
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import matplotlib.pyplot as plt
import os
import zipfile # Pour compresser les fichiers de mod√®le

# =========================
# === CONFIGURATION =======
# =========================

# D√©finition des chemins de fichiers et des colonnes
DATA_PATH = "medifex_data.csv" # Assurez-vous que ce fichier est √† la racine de votre d√©p√¥t GitHub
MODEL_DIR = "models" # Dossier pour sauvegarder les mod√®les
MODEL_PATH = os.path.join(MODEL_DIR, "medifex_model.pkl")
SCALER_PATH = os.path.join(MODEL_DIR, "scaler.pkl")
ENCODER_PATH = os.path.join(MODEL_DIR, "encoder.pkl")
# Ancien DEFAULT_CSV = "100kpatients.csv" - Si vous avez plusieurs datasets, g√©rez-les via uploader ou config

FEATURES = [
    'Age', 'Sexe', 'Tension_arterielle', 'Frequence_cardiaque', 'Cholesterol', 'Glycemie', 'BMI', 'SCFA', 'TMAO',
    'Indole', 'CRP', 'CYP2C19*2 (rs4244285)', 'TPMT*3C (rs1142345)', 'NAT2*6 (rs1799930)', 'SLCO1B1 (rs4149056)',
    'VKORC1 (rs9923231)', 'CYP2D6*4 (rs3892097)', 'Statut_Tabagique', 'Consommation_Alcool', 'Niveau_Activite_Physique'
]
TARGET = "Etat_Sante"

# =========================
# === FONCTIONS UTILITAIRES ===
# =========================

@st.cache_data # Cache la fonction pour ne la recharger qu'une fois ou si le contenu change
def load_data(source):
    """
    Charge les donn√©es depuis un fichier CSV.
    Peut prendre un chemin de fichier (str) ou un objet UploadedFile de Streamlit.
    """
    df = pd.DataFrame() # Initialise df √† un DataFrame vide par d√©faut

    if isinstance(source, str): # Si la source est un chemin de fichier (pour le chargement par d√©faut)
        if not os.path.exists(source):
            st.error(f"Fichier de donn√©es introuvable : {source}")
            return pd.DataFrame()
        try:
            df = pd.read_csv(source)
        except Exception as e:
            st.error(f"Erreur lors du chargement du fichier '{source}' : {e}")
            return pd.DataFrame()
    elif hasattr(source, 'read'): # Si la source est un objet fichier (comme UploadedFile)
        try:
            df = pd.read_csv(source)
        except Exception as e:
            st.error(f"Erreur lors du chargement du fichier t√©l√©vers√© : {e}")
            return pd.DataFrame()
    else:
        st.error("Source de donn√©es non reconnue. Veuillez fournir un chemin valide ou un fichier √† t√©l√©verser.")
        return pd.DataFrame()

    # Le traitement des colonnes est appliqu√© apr√®s le chargement, quel que soit le type de source
    if not df.empty:
        for col in ['Age', 'Tension_arterielle', 'Frequence_cardiaque', 'Cholesterol', 'Glycemie', 'BMI', 'SCFA', 'TMAO', 'Indole', 'CRP']:
            if col in df.columns:
                df[col] = pd.to_numeric(df[col], errors='coerce')
    return df

def train_and_evaluate_model(df, features, target, progress_callback=None):
    """
    Entra√Æne un mod√®le GradientBoostingClassifier avec GridSearchCV
    et √©value ses performances. Sauvegarde le mod√®le, scaler et encoder.
    """
    if df.empty or len(df) < 2:
        st.error("Donn√©es insuffisantes pour l'entra√Ænement du mod√®le.")
        return None, None, None, None

    # S√©paration des caract√©ristiques (X) et de la cible (y)
    X = df[features]
    y = df[target]

    # Encodage de la variable cible
    label_encoder = LabelEncoder()
    y_encoded = label_encoder.fit_transform(y)
    st.write(f"Classes encod√©es : {label_encoder.classes_}")

    # Encodage des caract√©ristiques cat√©gorielles (si elles existent et ne sont pas d√©j√† num√©riques)
    # Pour l'instant, toutes les FEATURES sont num√©riques ou seront trait√©es par LabelEncoder si elles sont d√©tect√©es comme objets
    # Si 'Sexe' est 'Homme'/'Femme', LabelEncoder le g√©rera. Si d'autres cat√©gorielles, il faudra un OneHotEncoder.
    for col in X.columns:
        if X[col].dtype == 'object' or X[col].dtype == 'category':
            X[col] = label_encoder.fit_transform(X[col]) # Utiliser un LabelEncoder s√©par√© si plusieurs cols cat√©gorielles

    # S√©paration des donn√©es en ensembles d'entra√Ænement et de test
    X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)

    # Mise √† l'√©chelle des caract√©ristiques num√©riques
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    # Conversion en DataFrame pour conserver les noms de colonnes (utile pour la pr√©diction)
    X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=features)
    X_test_scaled_df = pd.DataFrame(X_test_scaled, columns=features)

    # D√©finition du mod√®le et de la grille de param√®tres pour GridSearchCV
    model = GradientBoostingClassifier(random_state=42)
    param_grid = {
        'n_estimators': [100, 200, 300],
        'learning_rate': [0.01, 0.1, 0.2],
        'max_depth': [3, 4, 5]
    }

    st.info("Recherche des meilleurs hyperparam√®tres avec GridSearchCV (cela peut prendre du temps)...")
    grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, n_jobs=-1, verbose=1)
    grid_search.fit(X_train_scaled_df, y_train)

    best_model = grid_search.best_estimator_
    st.write(f"Meilleurs hyperparam√®tres trouv√©s : {grid_search.best_params_}")

    # Pr√©diction et √©valuation
    y_pred = best_model.predict(X_test_scaled_df)
    accuracy = accuracy_score(y_test, y_pred)
    report = classification_report(y_test, y_pred, target_names=label_encoder.classes_, output_dict=True)

    # Sauvegarde des mod√®les
    os.makedirs(MODEL_DIR, exist_ok=True) # Cr√©e le dossier 'models' si non existant
    joblib.dump(best_model, MODEL_PATH)
    joblib.dump(scaler, SCALER_PATH)
    joblib.dump(label_encoder, ENCODER_PATH)

    st.success("Mod√®le, scaler et encodeur sauvegard√©s localement.")

    return accuracy, report, best_model, label_encoder.classes_

# =========================
# === INTERFACE STREAMLIT ===
# =========================

st.set_page_config(
    page_title="M√©difex AI - Entra√Ænement",
    page_icon="üß†",
    layout="wide",
    initial_sidebar_state="expanded"
)

st.title("üß† Interface d'Entra√Ænement du Mod√®le M√©difex AI")

st.header("1. Chargement des Donn√©es")
uploaded_file = st.file_uploader("Chargez un fichier CSV pour l'entra√Ænement", type=["csv"])

df = pd.DataFrame()
if uploaded_file is not None:
    # Appelle la fonction load_data avec l'objet UploadedFile
    df = load_data(uploaded_file)
    if not df.empty:
        st.success(f"Fichier '{uploaded_file.name}' charg√© avec succ√®s. {len(df)} lignes trouv√©es.")
        st.subheader("Aper√ßu des donn√©es")
        st.dataframe(df.head())
        st.write("Statistiques descriptives :")
        st.dataframe(df.describe())
else:
    # Charger le fichier par d√©faut si aucun n'est upload√© et qu'il existe
    if os.path.exists(DATA_PATH):
        # Appelle la fonction load_data avec le chemin du fichier par d√©faut
        df = load_data(DATA_PATH)
        if not df.empty:
            st.info(f"Fichier de donn√©es par d√©faut '{DATA_PATH}' charg√©. {len(df)} lignes.")
            st.dataframe(df.head())
    else:
        st.warning("Veuillez charger un fichier CSV pour commencer l'entra√Ænement.")

st.header("2. Entra√Æner le Mod√®le IA")
st.info("L'entra√Ænement inclut la recherche des meilleurs hyperparam√®tres (GridSearchCV). Cela peut prendre du temps en fonction de la taille de vos donn√©es et de la puissance de calcul.")

# Placeholder pour la courbe d'apprentissage si on voulait la mettre √† jour en direct (moins pertinent avec GridSearchCV)
# curve_placeholder = st.empty()

if not df.empty and st.button("Lancer l'entra√Ænement et l'√©valuation"):
    st.write("---")
    st.subheader("Processus d'Entra√Ænement")
    status_placeholder = st.empty() # Pour afficher le statut de l'entra√Ænement
    
    accuracy, report_dict, best_model, target_classes = train_and_evaluate_model(
        df, FEATURES, TARGET, progress_callback=lambda x: status_placeholder.text(f"Progression : {x:.2%}")
    )

    if best_model is not None:
        st.subheader("R√©sultats de l'Entra√Ænement")
        st.write(f"**Pr√©cision (Accuracy) du meilleur mod√®le : {accuracy:.2%}**")

        if accuracy >= 0.95:
            st.success("F√©licitations ! Le mod√®le a atteint une pr√©cision de 95% ou plus.")
        else:
            st.warning(f"Le mod√®le a atteint une pr√©cision de {accuracy:.2%}. L'objectif de 95% n'est pas atteint.")
            st.info("Pour am√©liorer la pr√©cision :")
            st.write("- **Ajoutez plus de donn√©es** d'entra√Ænement.")
            st.write("- **Am√©liorez la qualit√© de vos donn√©es** (gestion des valeurs manquantes, outliers).")
            st.write("- **Faites de l'ing√©nierie de fonctionnalit√©s** (cr√©ez de nouvelles features √† partir des existantes).")
            st.write("- **Essayez d'autres mod√®les** ou ajustez davantage la grille de param√®tres.")

        st.subheader("Rapport de Classification D√©taill√©")
        st.json(report_dict) # Affiche le rapport sous forme de JSON pour une meilleure lisibilit√©
        
        # Affichage de la matrice de confusion
        # Pour une matrice de confusion pertinente, il faut utiliser les donn√©es de test, ou bien re-pr√©dire sur l'ensemble complet si le mod√®le est finalis√©
        # Je vais utiliser l'ensemble test pour la matrice de confusion pour rester coh√©rent avec l'√©valuation
        X_temp = df[FEATURES]
        y_temp_encoded = LabelEncoder().fit_transform(df[TARGET])
        X_train_temp, X_test_temp, y_train_temp, y_test_temp = train_test_split(X_temp, y_temp_encoded, test_size=0.2, random_state=42)
        
        scaler_temp = StandardScaler()
        X_test_scaled_temp = scaler_temp.fit(X_train_temp).transform(X_test_temp) # Scale avec le scaler fit sur le train
        
        y_pred_cm = best_model.predict(X_test_scaled_temp)
        cm = confusion_matrix(y_test_temp, y_pred_cm, labels=[i for i in range(len(target_classes))])
        
        st.write("Matrice de Confusion (sur l'ensemble de test) :")
        cm_df = pd.DataFrame(cm, index=[f"Vrai: {c}" for c in target_classes], columns=[f"Pr√©dit: {c}" for c in target_classes])
        st.dataframe(cm_df)

    st.write("---")

st.header("3. T√©l√©charger les Mod√®les Entra√Æn√©s")
st.info("Une fois l'entra√Ænement termin√©, t√©l√©chargez les mod√®les pour les inclure dans votre application Streamlit principale (`medifex_ia.py`).")

# V√©rifier si les mod√®les existent avant de proposer le t√©l√©chargement
if os.path.exists(MODEL_PATH) and os.path.exists(SCALER_PATH) and os.path.exists(ENCODER_PATH):
    zip_file_name = "medifex_model_and_scaler.zip"
    with zipfile.ZipFile(zip_file_name, 'w') as zipf:
        zipf.write(MODEL_PATH, os.path.basename(MODEL_PATH))
        zipf.write(SCALER_PATH, os.path.basename(SCALER_PATH))
        zipf.write(ENCODER_PATH, os.path.basename(ENCODER_PATH))

    with open(zip_file_name, "rb") as f:
        st.download_button(
            label="üì• T√©l√©charger les mod√®les (mod√®le, scaler, encodeur)",
            data=f.read(),
            file_name=zip_file_name,
            mime="application/zip"
        )
    st.success(f"Les mod√®les sont pr√™ts √† √™tre t√©l√©charg√©s depuis le dossier '{MODEL_DIR}'. N'oubliez pas de les committer sur GitHub si vous voulez que votre application principale (`medifex_ia.py`) les utilise.")
else:
    st.warning("Aucun mod√®le entra√Æn√© √† t√©l√©charger pour le moment. Lancez l'entra√Ænement ci-dessus.")

st.write("---")
st.info("D√©velopp√© avec ‚ù§Ô∏è par Gemini pour M√©difex")
