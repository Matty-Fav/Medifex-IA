import streamlit as st
import pandas as pd
import numpy as np
import joblib
from datetime import datetime
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import matplotlib.pyplot as plt
import os
import zipfile

# =========================
# === CONFIGURATION =======
# =========================

# D√©finition des chemins de fichiers et des colonnes
DATA_PATH = "medifex_data.csv" # Assurez-vous que ce fichier est √† la racine de votre d√©p√¥t GitHub
MODEL_DIR = "models" # Dossier pour sauvegarder les mod√®les
MODEL_PATH = os.path.join(MODEL_DIR, "medifex_model.pkl")
SCALER_PATH = os.path.join(MODEL_DIR, "scaler.pkl")
ENCODER_PATH = os.path.join(MODEL_DIR, "encoder.pkl")

FEATURES = [
    'Age', 'Sexe', 'Tension_arterielle', 'Frequence_cardiaque', 'Cholesterol', 'Glycemie', 'BMI', 'SCFA', 'TMAO',
    'Indole', 'CRP', 'CYP2C19*2 (rs4244285)', 'TPMT*3C (rs1142345)', 'NAT2*6 (rs1799930)', 'SLCO1B1 (rs4149056)',
    'VKORC1 (rs9923231)', 'CYP2D6*4 (rs3892097)', 'Statut_Tabagique', 'Consommation_Alcool', 'Niveau_Activite_Physique'
]
TARGET = "Etat_Sante"

# =========================
# === FONCTIONS UTILITAIRES ===
# =========================

@st.cache_data
def load_data(source):
    """
    Charge les donn√©es depuis un fichier CSV.
    Peut prendre un chemin de fichier (str) ou un objet UploadedFile de Streamlit.
    """
    df = pd.DataFrame()

    if isinstance(source, str):
        if not os.path.exists(source):
            st.error(f"Fichier de donn√©es introuvable : {source}")
            return pd.DataFrame()
        try:
            df = pd.read_csv(source)
        except Exception as e:
            st.error(f"Erreur lors du chargement du fichier '{source}' : {e}")
            return pd.DataFrame()
    elif hasattr(source, 'read'):
        try:
            df = pd.read_csv(source)
        except Exception as e:
            st.error(f"Erreur lors du chargement du fichier t√©l√©vers√© : {e}")
            return pd.DataFrame()
    else:
        st.error("Source de donn√©es non reconnue. Veuillez fournir un chemin valide ou un fichier √† t√©l√©verser.")
        return pd.DataFrame()

    if not df.empty:
        for col in ['Age', 'Tension_arterielle', 'Frequence_cardiaque', 'Cholesterol', 'Glycemie', 'BMI', 'SCFA', 'TMAO', 'Indole', 'CRP']:
            if col in df.columns:
                df[col] = pd.to_numeric(df[col], errors='coerce')
    return df

def train_and_evaluate_model(df, features, target, status_placeholder): # Renomm√© pour √™tre plus clair
    """
    Entra√Æne un mod√®le GradientBoostingClassifier avec GridSearchCV
    et √©value ses performances. Sauvegarde le mod√®le, scaler et encoder.
    """
    if df.empty or len(df) < 2:
        status_placeholder.error("Donn√©es insuffisantes pour l'entra√Ænement du mod√®le.")
        return None, None, None, None

    status_placeholder.info("√âtape 1/5: Nettoyage et v√©rification des donn√©es...")
    initial_rows = len(df)
    cols_to_check = [col for col in features + [target] if col in df.columns]
    df.dropna(subset=cols_to_check, inplace=True)
    rows_after_dropna = len(df)

    if initial_rows > rows_after_dropna:
        status_placeholder.warning(f"Attention : {initial_rows - rows_after_dropna} lignes ont √©t√© supprim√©es car elles contenaient des valeurs manquantes dans les features ou la cible.")
    
    if rows_after_dropna < 2:
        status_placeholder.error("Donn√©es insuffisantes pour l'entra√Ænement apr√®s la suppression des valeurs manquantes.")
        return None, None, None, None

    X = df[features]
    y = df[target]

    if len(y.unique()) < 2:
        status_placeholder.error(f"La variable cible '{target}' contient moins de 2 classes uniques ({y.unique()}) apr√®s le nettoyage. Impossible de r√©aliser une classification.")
        return None, None, None, None

    status_placeholder.info("√âtape 2/5: Encodage des variables...")
    label_encoder = LabelEncoder()
    y_encoded = label_encoder.fit_transform(y)
    st.write(f"Classes encod√©es pour '{target}' : {label_encoder.classes_}")

    for col in X.columns:
        if X[col].dtype == 'object' or X[col].dtype == 'category':
            try:
                temp_encoder = LabelEncoder() 
                X[col] = temp_encoder.fit_transform(X[col])
            except Exception as e:
                status_placeholder.warning(f"Impossible d'encoder la colonne cat√©gorielle '{col}' : {e}. Assurez-vous que ses valeurs sont coh√©rentes.")
    
    status_placeholder.info("√âtape 3/5: S√©paration et mise √† l'√©chelle des donn√©es...")
    X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)

    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=features)
    X_test_scaled_df = pd.DataFrame(X_test_scaled, columns=features)
    
    if X_train_scaled_df.isnull().sum().sum() > 0:
        status_placeholder.error(f"Des valeurs NaN subsistent dans les donn√©es d'entra√Ænement apr√®s mise √† l'√©chelle. Impossible de continuer.")
        st.dataframe(X_train_scaled_df.isnull().sum())
        return None, None, None, None
    if not np.isfinite(X_train_scaled_df).all().all():
        status_placeholder.error(f"Des valeurs infinies subsistent dans les donn√©es d'entra√Ænement apr√®s mise √† l'√©chelle. Impossible de continuer.")
        return None, None, None, None

    status_placeholder.info("√âtape 4/5: Entra√Ænement du mod√®le avec GridSearchCV (cela peut prendre du temps)...")
    model = GradientBoostingClassifier(random_state=42)
    param_grid = {
        'n_estimators': [100, 200, 300],
        'learning_rate': [0.01, 0.1, 0.2],
        'max_depth': [3, 4, 5]
    }

    # Utilisation de st.spinner pour une indication visuelle d'une longue op√©ration
    with st.spinner("Entra√Ænement en cours, veuillez patienter..."):
        grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, n_jobs=-1, verbose=1)
        grid_search.fit(X_train_scaled_df, y_train)

    best_model = grid_search.best_estimator_
    st.write(f"Meilleurs hyperparam√®tres trouv√©s : {grid_search.best_params_}")

    status_placeholder.info("√âtape 5/5: √âvaluation du mod√®le et sauvegarde...")
    y_pred = best_model.predict(X_test_scaled_df)
    accuracy = accuracy_score(y_test, y_pred)
    report = classification_report(y_test, y_pred, target_names=label_encoder.classes_, output_dict=True)

    os.makedirs(MODEL_DIR, exist_ok=True)
    joblib.dump(best_model, MODEL_PATH)
    joblib.dump(scaler, SCALER_PATH)
    joblib.dump(label_encoder, ENCODER_PATH)

    status_placeholder.success("Mod√®le, scaler et encodeur sauvegard√©s localement. Entra√Ænement termin√©!")
    
    return accuracy, report, best_model, label_encoder.classes_

# =========================
# === INTERFACE STREAMLIT ===
# =========================

st.set_page_config(
    page_title="M√©difex AI - Entra√Ænement",
    page_icon="üß†",
    layout="wide",
    initial_sidebar_state="expanded"
)

st.title("üß† Interface d'Entra√Ænement du Mod√®le M√©difex AI")

st.header("1. Chargement des Donn√©es")
uploaded_file = st.file_uploader("Chargez un fichier CSV pour l'entra√Ænement", type=["csv"])

df = pd.DataFrame()
if uploaded_file is not None:
    df = load_data(uploaded_file)
    if not df.empty:
        st.success(f"Fichier '{uploaded_file.name}' charg√© avec succ√®s. {len(df)} lignes trouv√©es.")
        st.subheader("Aper√ßu des donn√©es")
        st.dataframe(df.head())
        st.write("Statistiques descriptives :")
        st.dataframe(df.describe())
else:
    if os.path.exists(DATA_PATH):
        df = load_data(DATA_PATH)
        if not df.empty:
            st.info(f"Fichier de donn√©es par d√©faut '{DATA_PATH}' charg√©. {len(df)} lignes.")
            st.dataframe(df.head())
    else:
        st.warning("Veuillez charger un fichier CSV pour commencer l'entra√Ænement.")

st.header("2. Entra√Æner le Mod√®le IA")
st.info("L'entra√Ænement inclut la recherche des meilleurs hyperparam√®tres (GridSearchCV). Cela peut prendre du temps en fonction de la taille de vos donn√©es et de la puissance de calcul.")

# Placeholder pour les messages d'√©tat en direct
training_status_placeholder = st.empty()

if not df.empty and st.button("Lancer l'entra√Ænement et l'√©valuation"):
    st.write("---")
    st.subheader("Processus d'Entra√Ænement")
    
    # Passe le placeholder √† la fonction d'entra√Ænement
    accuracy, report_dict, best_model, target_classes = train_and_evaluate_model(
        df, FEATURES, TARGET, training_status_placeholder
    )

    if best_model is not None:
        st.subheader("R√©sultats de l'Entra√Ænement")
        st.write(f"**Pr√©cision (Accuracy) du meilleur mod√®le : {accuracy:.2%}**")

        if accuracy >= 0.95:
            st.success("F√©licitations ! Le mod√®le a atteint une pr√©cision de 95% ou plus.")
        else:
            st.warning(f"Le mod√®le a atteint une pr√©cision de {accuracy:.2%}. L'objectif de 95% n'est pas atteint.")
            st.info("Pour am√©liorer la pr√©cision :")
            st.write("- **Ajoutez plus de donn√©es** d'entra√Ænement.")
            st.write("- **Am√©liorez la qualit√© de vos donn√©es** (gestion des valeurs manquantes, outliers).")
            st.write("- **Faites de l'ing√©nierie de fonctionnalit√©s** (cr√©ez de nouvelles features √† partir des existantes).")
            st.write("- **Essayez d'autres mod√®les** ou ajustez davantage la grille de param√®tres.")

        st.subheader("Rapport de Classification D√©taill√©")
        st.json(report_dict)
        
        st.write("Matrice de Confusion (sur l'ensemble de test) :")
        X_temp = df[FEATURES]
        y_temp_encoded = LabelEncoder().fit_transform(df[TARGET])
        X_train_temp, X_test_temp, y_train_temp, y_test_temp = train_test_split(X_temp, y_temp_encoded, test_size=0.2, random_state=42)
        
        scaler_temp = StandardScaler()
        X_test_scaled_temp = scaler_temp.fit(X_train_temp).transform(X_test_temp) 
        
        y_pred_cm = best_model.predict(X_test_scaled_temp)
        cm = confusion_matrix(y_test_temp, y_pred_cm, labels=[i for i in range(len(target_classes))])
        
        st.write("Matrice de Confusion (sur l'ensemble de test) :")
        cm_df = pd.DataFrame(cm, index=[f"Vrai: {c}" for c in target_classes], columns=[f"Pr√©dit: {c}" for c in target_classes])
        st.dataframe(cm_df)

    st.write("---")

st.header("3. T√©l√©charger les Mod√®les Entra√Æn√©s")
st.info("Une fois l'entra√Ænement termin√©, t√©l√©chargez les mod√®les pour les inclure dans votre application Streamlit principale (`medifex_ia.py`).")

if os.path.exists(MODEL_PATH) and os.path.exists(SCALER_PATH) and os.path.exists(ENCODER_PATH):
    zip_file_name = "medifex_model_and_scaler.zip"
    with zipfile.ZipFile(zip_file_name, 'w') as zipf:
        zipf.write(MODEL_PATH, os.path.basename(MODEL_PATH))
        zipf.write(SCALER_PATH, os.path.basename(SCALER_PATH))
        zipf.write(ENCODER_PATH, os.path.basename(ENCODER_PATH))

    with open(zip_file_name, "rb") as f:
        st.download_button(
            label="üì• T√©l√©charger les mod√®les (mod√®le, scaler, encodeur)",
            data=f.read(),
            file_name=zip_file_name,
            mime="application/zip"
        )
    st.success(f"Les mod√®les sont pr√™ts √† √™tre t√©l√©charg√©s depuis le dossier '{MODEL_DIR}'. N'oubliez pas de les committer sur GitHub si vous voulez que votre application principale (`medifex_ia.py`) les utilise.")
else:
    st.warning("Aucun mod√®le entra√Æn√© √† t√©l√©charger pour le moment. Lancez l'entra√Ænement ci-dessus.")

st.write("---")
st.info("D√©velopp√© avec ‚ù§Ô∏è par Gemini pour M√©difex")
